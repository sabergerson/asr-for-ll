{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b817eff-93d3-44f0-9fbe-e926b8d6a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face\n",
    "os.environ['HUGGINGFACE_HUB_TOKEN'] = '####'\n",
    "login(token=os.environ['HUGGINGFACE_HUB_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d16326ac-ba17-41d0-94bb-7873901a9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data path\n",
    "data_dir = 'edacc_v1.0/'\n",
    "\n",
    "# Load accent info\n",
    "acc_df = pd.read_csv(data_dir+'linguistic_background.csv')\n",
    "acc_df.rename(columns={'How would you describe your accent in English? (e.g. Italian, Glaswegian)': 'Accent'}, inplace=True)\n",
    "acc_df = acc_df[['CONVERSATION_ID', 'PARTICIPANT_ID', 'Accent']]\n",
    "\n",
    "# Map to standardized categories (based on data inspection)\n",
    "accent_map = {\n",
    "    'American': [\n",
    "        'Slightly American', 'Mostly West Coast American with some Australian Intonation', 'American accent ',\n",
    "        'American, I guess.', 'American ', 'American with a slight accent', 'American-ish', 'Midwestern United States'],\n",
    "    'Jamaican': ['Jamaican ', 'Jamaican accent ', 'EDACC-C52-A', 'EDACC-C52-B'],\n",
    "    'English': ['South London', 'Southern London', 'English ', 'British', 'English with Scottish inflections'],\n",
    "    'Scottish': ['Scottish (Fife)', 'Glaswegian (not slang)', 'Glaswegian'],\n",
    "    'Irish': ['Irish/ Dublin', 'South Dublin Irish', 'Southern Irish'],\n",
    "    'French': ['EDACC-C18-A'],\n",
    "    'Spanish': ['Spanish accent', 'Spanish American'],\n",
    "    'Italian': [\n",
    "        'italian mixed with American and British English ', 'Italian mixed with American accent',\n",
    "        'italian', 'Neutral English, Italian'],\n",
    "    'Lithuanian': ['Lithuanian (eastern European)'],\n",
    "    'Romanian': ['Romanian '],\n",
    "    'Polish': ['European', 'EDACC-C24-B'],\n",
    "    'Eastern European': ['East-European', 'Neutral accent'],\n",
    "    'Chinese': ['Chinese accent or mixed accent(US, UK, China..) perhaps', 'Chinese '],\n",
    "    'Vietnamese': ['Vietnamese accent', 'Slight Vietnamese accent', 'Vietnamese English', 'EDACC-C59-A'],\n",
    "    'Indian': ['Standard Indian English', 'Indian ', 'Neutral'],\n",
    "    'Pakistani': ['Indian / Pakistani accent', 'Pakistani/American'],\n",
    "    'Egyptian': ['Egyptian '],\n",
    "    'Nigerian': ['Afrian', 'EDACC-C45-A'],\n",
    "    'Ghanaian': ['Ghanaian ', 'EDACC-C45-B'],\n",
    "    'Kenyan': ['African accent'],\n",
    "    'South African': ['South African English'],\n",
    "    'Brazilian': ['Brazilian accent'],\n",
    "    'Ecuadorian': ['Latin American'],\n",
    "    'Chilean': ['South American'],\n",
    "    'Colombian': ['Lat√≠n American', 'Latin'],\n",
    "    'International': ['Trans-Atlantic', 'Generic middle class white person ', 'Standard American,Scottish', 'North American'],\n",
    "    'Sri Lankan': ['Asian', 'EDACC-C10-B'],\n",
    "    'Russian': ['EDACC-C24-A'],\n",
    "    'Filipino': ['European']\n",
    "}\n",
    "\n",
    "# Create reverse mapping for lookup\n",
    "reverse_map = {}\n",
    "for accent, values in accent_map.items():\n",
    "    for value in values:\n",
    "        reverse_map[value] = accent\n",
    "\n",
    "# Update accent using reverse mapping\n",
    "def update_accent(row):\n",
    "    participant_id = row['PARTICIPANT_ID']\n",
    "    accent = row['Accent']\n",
    "    if accent in reverse_map:\n",
    "        return reverse_map[accent]\n",
    "    elif participant_id in reverse_map:\n",
    "        return reverse_map[participant_id]\n",
    "    else:\n",
    "        return accent\n",
    "\n",
    "# Apply accent map to data\n",
    "acc_df['Accent'] = acc_df.apply(update_accent, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca3d062-369a-4b0d-9082-4a63894cab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data paths\n",
    "dev = data_dir+'dev/'\n",
    "test = data_dir+'test/'\n",
    "data = data_dir+'data'\n",
    "\n",
    "# Read in CSV files\n",
    "def read(filen):\n",
    "  with open(filen, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "  return lines\n",
    "\n",
    "# Split transcripts into features\n",
    "def split_trans(line):\n",
    "    parts = line.strip().split(' ', 1)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "# Split segments into features\n",
    "def split_seg(line):\n",
    "    parts = line.strip().split(' ')\n",
    "    return parts[0], parts[1], parts[2], parts[3]\n",
    "\n",
    "# Create transcript dataframe\n",
    "def make_trans_df(file):\n",
    "    return pd.DataFrame([split_trans(line) for line in read(file)], columns=['Code', 'Transcript'])\n",
    "\n",
    "# Create segment dataframe\n",
    "def make_seg_df(file):\n",
    "    return pd.DataFrame([split_seg(line) for line in read(file)], columns=['Code', 'File', 'Start', 'End'])\n",
    "\n",
    "# Function to process files\n",
    "def process_files(folder):\n",
    "\n",
    "    # Create dataframes for transcripts and segments\n",
    "    trans_df = make_trans_df(folder+'text.csv')\n",
    "    seg_df = make_seg_df(folder+'segments.csv')\n",
    "\n",
    "    # Lists to hold data\n",
    "    codes = []\n",
    "    audio_paths = []\n",
    "    trans_list = []\n",
    "    accent_list = []\n",
    "\n",
    "    for file in os.listdir(data):\n",
    "\n",
    "        # Get audio file path\n",
    "        audio_path = os.path.join(data, file)\n",
    "        audio_file = os.path.splitext(file)[0]\n",
    "\n",
    "        if audio_file in seg_df['File'].values:\n",
    "\n",
    "            # Load audio file\n",
    "            audio, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "            # Get corresponding segment data\n",
    "            file_segs = seg_df[seg_df['File'] == audio_file]\n",
    "            segs = file_segs[['Code', 'File', 'Start', 'End']].values.tolist()\n",
    "\n",
    "            i = 0\n",
    "            for seg in segs:\n",
    "\n",
    "                # Get start and end times\n",
    "                code = seg[0]\n",
    "                start = float(seg[2])\n",
    "                end = float(seg[3])\n",
    "\n",
    "                # Convert times to samples\n",
    "                start_sample = int(start * sr)\n",
    "                end_sample = int(end * sr)\n",
    "\n",
    "                # Extract audio segment\n",
    "                audio_seg = audio[start_sample:end_sample]\n",
    "\n",
    "                # Save new audio segment\n",
    "                audio_seg_filen = f'{code}.wav'\n",
    "                audio_seg_path = os.path.join(data_dir+'segments', audio_seg_filen)\n",
    "                sf.write(audio_seg_path, audio_seg, sr)\n",
    "\n",
    "                # Get corresponding transcript\n",
    "                transcript = trans_df[trans_df['Code'] == code]['Transcript'].values[0]\n",
    "\n",
    "                # Get corresponding accent\n",
    "                participant_id = f\"{code[:9]}-{'A' if i%2 == 0 else 'B'}\"\n",
    "                accent = acc_df.loc[acc_df['PARTICIPANT_ID'] == participant_id, 'Accent'].iloc[0]\n",
    "                i += 1\n",
    "\n",
    "                # Save data\n",
    "                codes.append(code)\n",
    "                audio_paths.append(audio_seg_path)\n",
    "                trans_list.append(transcript)\n",
    "                accent_list.append(accent)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data_lists = {\n",
    "        'code': codes,\n",
    "        'audio': audio_paths,\n",
    "        'transcript': trans_list,\n",
    "        'accent': accent_list\n",
    "    }\n",
    "    df = pd.DataFrame(data_lists)\n",
    "\n",
    "    # Convert DataFrames to HuggingFace Datasets\n",
    "    dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "    # Define the audio feature\n",
    "    dataset = dataset.cast_column('audio', Audio())\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29bd43c5-fe2a-4769-9d1f-e7a43593ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'dev': process_files(dev),\n",
    "    'test': process_files(test)\n",
    "})\n",
    "\n",
    "# Push to HuggingFace Hub\n",
    "dataset_dict.push_to_hub('sage-bergerson/edacc_whisper')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
